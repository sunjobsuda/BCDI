{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "traindataFile = u\"E:/BDCI Match/train_preproceed.csv\"\n",
    "\n",
    "df = pd.read_csv(traindataFile)\n",
    "\n",
    "#对预测标签重新编码\n",
    "le = LabelEncoder()\n",
    "#获取预测结果的所有取值\n",
    "res_labels = df[\"current_service\"].unique()\n",
    "\n",
    "le.fit(res_labels)\n",
    "df[\"res\"] = le.transform(df[\"current_service\"])\n",
    "\n",
    "features_selected =  [\"service_type\",\"online_time\",\"1_total_fee\",\"2_total_fee\", \"3_total_fee\", \"4_total_fee\",\n",
    "                     \"month_traffic\", \"many_over_bill\", \"contract_type\",\"contract_time\",\"pay_num\",\"last_month_traffic\",\n",
    "                     \"local_trafffic_month\",\"local_caller_time\",\"service1_caller_time\",\"service2_caller_time\", \"age\"]\n",
    "\n",
    "param_grid = {\n",
    "    \"boosting_type\" : ['gbdt','dart'],\n",
    "    \"\"\n",
    "}\n",
    "muti_results = lgb.LGBMClassifier(\n",
    "   ,\n",
    "    num_leaves = 39,\n",
    "    max_depth = 6,\n",
    "    learning_rate = 0.02,\n",
    "    n_estimators = 10000,\n",
    "    max_bin = 255,\n",
    "    subsample_for_bin = 200000,\n",
    "    objective = 'multiclass',\n",
    "    min_split_gain = 0.0,\n",
    "    min_child_weight = 0.001,\n",
    "    min_child_samples = 20,\n",
    "    subsample = 1.0,\n",
    "    subsample_freq = 1,\n",
    "    colsample_bytree = 1.0,\n",
    "    reg_alpha = 0.0,\n",
    "    reg_lambda = 0.0,\n",
    "    random_state = 0,\n",
    "    n_jobs = 3,\n",
    "    silent = True,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=100,\n",
    "    show_stdv=True)\n",
    "muti_results.fit(X_train,\n",
    "                 y_train,\n",
    "                 eval_set=[(X_test, y_test)],\n",
    "                 early_stopping_rounds = 200\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.create_tree_digraph(lgb_clf, tree_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据包\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 基础配置信息\n",
    "path = 'E:/BDCI Match/'\n",
    "n_splits = 5\n",
    "seed = 42\n",
    "\n",
    "# lgb 参数\n",
    "params={\n",
    "    \"learning_rate\":0.1,\n",
    "    \"lambda_l1\":0.1,\n",
    "    \"lambda_l2\":0.2,\n",
    "    \"max_depth\":4,\n",
    "    \"objective\":\"multiclass\",\n",
    "    \"num_class\":15,\n",
    "    \"silent\":True,\n",
    "}\n",
    "\n",
    "# 读取数据\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "test = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "'''\n",
    "简单分析数据：\n",
    "user_id 为编码后的数据，大小：\n",
    "train data shape (612652, 27)\n",
    "train data of user_id shape 612652\n",
    "简单的1个用户1条样本的题目,标签的范围 current_service\n",
    "'''\n",
    "print('标签',set(train.columns)-set(test.columns))\n",
    "\n",
    "print('train data shape',train.shape)\n",
    "print('train data of user_id shape',len(set(train['user_id'])))\n",
    "print('train data of current_service shape',(set(train['current_service'])))\n",
    "\n",
    "print('train data shape',test.shape)\n",
    "print('train data of user_id shape',len(set(test['user_id'])))\n",
    "\n",
    "# 对标签编码 映射关系\n",
    "label2current_service = dict(zip(range(0,len(set(train['current_service']))),sorted(list(set(train['current_service'])))))\n",
    "current_service2label = dict(zip(sorted(list(set(train['current_service']))),range(0,len(set(train['current_service'])))))\n",
    "\n",
    "# 原始数据的标签映射\n",
    "train['current_service'] = train['current_service'].map(current_service2label)\n",
    "\n",
    "features_selected =  [\"service_type\",\"online_time\",\"1_total_fee\",\"2_total_fee\", \"3_total_fee\", \"4_total_fee\",\n",
    "                     \"month_traffic\", \"many_over_bill\", \"contract_type\",\"contract_time\",\"pay_num\",\"last_month_traffic\",\n",
    "                     \"local_trafffic_month\",\"local_caller_time\",\"service1_caller_time\",\"service2_caller_time\", \"age\"]\n",
    "\n",
    "X = train[features_selected]\n",
    "y = train.pop('current_service')\n",
    "X_test = test[features_selected]\n",
    "train_id = train.pop('user_id')\n",
    "test_id = test['user_id']\n",
    "train_col = train.columns\n",
    "\n",
    "# 数据有问题数据\n",
    "for i in features_selected:\n",
    "    X[i] = X[i].replace(\"\\\\N\",-1)\n",
    "    X_test[i] = X_test[i].replace(\"\\\\N\",-1)\n",
    "    \n",
    "# # 构造原始数据\n",
    "# y = train.pop('current_service')\n",
    "# train_id = train.pop('user_id')\n",
    "# # 这个字段有点问题\n",
    "# X = train\n",
    "# train_col = train.columns\n",
    "\n",
    "# X_test = test[train_col]\n",
    "# test_id = test['user_id']\n",
    "\n",
    "# # 数据有问题数据\n",
    "# for i in train_col:\n",
    "#     X[i] = X[i].replace(\"\\\\N\",-1)\n",
    "#     X_test[i] = X_test[i].replace(\"\\\\N\",-1)\n",
    "\n",
    "X,y,X_test = X.values,y,X_test.values\n",
    "\n",
    "# 采取k折模型方案\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 自定义F1评价函数\n",
    "def f1_score_vali(preds, data_vali):\n",
    "    labels = data_vali.get_label()\n",
    "    preds = np.argmax(preds.reshape(15, -1), axis=0)\n",
    "    score_vali = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_score', score_vali, True\n",
    "\n",
    "xx_score = []\n",
    "cv_pred = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits,random_state=seed,shuffle=True)\n",
    "for index,(train_index,test_index) in enumerate(skf.split(X,y)):\n",
    "    print(index)\n",
    "\n",
    "    X_train,X_valid,y_train,y_valid = X[train_index],X[test_index],y[train_index],y[test_index]\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    validation_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "    clf=lgb.train(params,train_data,num_boost_round=100000,valid_sets=[validation_data],early_stopping_rounds=50,feval=f1_score_vali,verbose_eval=1)\n",
    "\n",
    "    xx_pred = clf.predict(X_valid,num_iteration=clf.best_iteration)\n",
    "\n",
    "    xx_pred = [np.argmax(x) for x in xx_pred]\n",
    "\n",
    "    xx_score.append(f1_score(y_valid,xx_pred,average='weighted'))\n",
    "\n",
    "    y_test = clf.predict(X_test,num_iteration=clf.best_iteration)\n",
    "\n",
    "    y_test = [np.argmax(x) for x in y_test]\n",
    "\n",
    "    if index == 0:\n",
    "        cv_pred = np.array(y_test).reshape(-1, 1)\n",
    "    else:\n",
    "        cv_pred = np.hstack((cv_pred, np.array(y_test).reshape(-1, 1)))\n",
    "\n",
    "# 投票\n",
    "submit = []\n",
    "for line in cv_pred:\n",
    "    submit.append(np.argmax(np.bincount(line)))\n",
    "\n",
    "# 保存结果\n",
    "df_test = pd.DataFrame()\n",
    "df_test['id'] = list(test_id.unique())\n",
    "df_test['predict'] = submit\n",
    "df_test['predict'] = df_test['predict'].map(label2current_service)\n",
    "\n",
    "df_test.to_csv('E:/BDCI Match/result/baseline2.csv',index=False)\n",
    "\n",
    "print(xx_score,np.mean(xx_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Booster' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e4ce79c47429>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Booster' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape (612652, 27)\n",
      "train data of user_id shape 612652\n",
      "train data of current_service shape {89016259, 89016252, 90155946, 99999825, 90063345, 99104722, 99999828, 99999826, 89950166, 89950167, 89950168, 99999827, 99999830, 90109916, 89016253}\n",
      "Fitting 2 folds for each of 1152 candidates, totalling 2304 fits\n"
     ]
    }
   ],
   "source": [
    "# 导入数据包\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 自定义F1评价函数\n",
    "def f1_score_vali(preds, data_vali):\n",
    "    labels = data_vali.get_label()\n",
    "    preds = np.argmax(preds.reshape(15, -1), axis=0)\n",
    "    score_vali = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_score', score_vali, True\n",
    "\n",
    "# 基础配置信息\n",
    "path = 'E:/BDCI Match/'\n",
    "n_splits = 5\n",
    "seed = 42\n",
    "\n",
    "# 读取数据\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "\n",
    "'''\n",
    "简单分析数据：\n",
    "user_id 为编码后的数据，大小：\n",
    "train data shape (612652, 27)\n",
    "train data of user_id shape 612652\n",
    "简单的1个用户1条样本的题目,标签的范围 current_service\n",
    "'''\n",
    "\n",
    "print('train data shape',train.shape)\n",
    "print('train data of user_id shape',len(set(train['user_id'])))\n",
    "print('train data of current_service shape',(set(train['current_service'])))\n",
    "\n",
    "# 对标签编码 映射关系\n",
    "label2current_service = dict(zip(range(0,len(set(train['current_service']))),sorted(list(set(train['current_service'])))))\n",
    "current_service2label = dict(zip(sorted(list(set(train['current_service']))),range(0,len(set(train['current_service'])))))\n",
    "\n",
    "# 原始数据的标签映射\n",
    "train['current_service'] = train['current_service'].map(current_service2label)\n",
    "\n",
    "\n",
    "# 构造原始数据\n",
    "y = train.pop('current_service')\n",
    "train_id = train.pop('user_id')\n",
    "# 这个字段有点问题\n",
    "X = train\n",
    "train_col = train.columns\n",
    "\n",
    "# 数据有问题数据\n",
    "for i in train_col:\n",
    "    X[i] = X[i].replace(\"\\\\N\",-1)\n",
    "\n",
    "X,y = X.values,y\n",
    "\n",
    "lg = lgb.LGBMClassifier(\n",
    "    max_bin = 255,\n",
    "    subsample_for_bin = 200000,\n",
    "    objective = 'multiclass',\n",
    "    num_class =15,\n",
    "    min_split_gain = 0.0,\n",
    "    min_child_weight = 0.001,\n",
    "    min_child_samples = 20,\n",
    "    subsample = 1.0,\n",
    "    subsample_freq = 1,\n",
    "    reg_alpha = 0.0,\n",
    "    reg_lambda = 0.0,\n",
    "    random_state = 0\n",
    ")\n",
    "\n",
    "param_dict = {\n",
    "    \"boosting_type\" : ['gbdt','dart'],\n",
    "    \"max_depth\": [5, 8, 10, 20],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"num_leaves\": [40, 50, 100, 200],\n",
    "    \"n_estimators\": [3000, 5000, 10000, 20000],\n",
    "    \"colsample_bytree\": [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(lg, n_jobs=3, param_grid=param_dict, cv = 2, scoring='roc_auc', verbose=5)\n",
    "                           \n",
    "grid.fit(X,y)\n",
    "\n",
    "bestModel = grid.best_estimator_\n",
    "\n",
    "bestParam = grid.best_params_ \n",
    "\n",
    "print(\"bestParam; \")\n",
    "print(bestParam)\n",
    "\n",
    "bestScore = grid.best_score_ \n",
    "\n",
    "print(\"bestScore; \")\n",
    "print(bestScore)\n",
    "\n",
    "bestModel.save_model('./Model.txt')\n",
    "\n",
    "# 保存结果\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "for key in bestParam.keys():\n",
    "    t = []\n",
    "    t.append(bestParam[key])\n",
    "    df_test[key] = t\n",
    "\n",
    "df_test.to_csv('./Para.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-cfbf87626e0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbestParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbestParam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbestParam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'E:/BDCI Match/result/test.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bestParam = {\"A\":0, \"B\":1, \"C\":2}\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "print(bestParam)\n",
    "\n",
    "\n",
    "df_test.to_csv('E:/BDCI Match/result/test.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据包\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 基础配置信息\n",
    "path = 'E:/BDCI Match/'\n",
    "n_splits = 5\n",
    "seed = 42\n",
    "\n",
    "# 自定义F1评价函数\n",
    "def f1_score_vali(preds, data_vali):\n",
    "    labels = data_vali.get_label()\n",
    "    preds = np.argmax(preds.reshape(15, -1), axis=0)\n",
    "    score_vali = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_score', score_vali, True\n",
    "\n",
    "# 读取数据\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "test = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "# '''\n",
    "# 简单分析数据：\n",
    "# user_id 为编码后的数据，大小：\n",
    "# train data shape (612652, 27)\n",
    "# train data of user_id shape 612652\n",
    "# 简单的1个用户1条样本的题目,标签的范围 current_service\n",
    "# '''\n",
    "# print('标签',set(train.columns)-set(test.columns))\n",
    "\n",
    "# print('train data shape',train.shape)\n",
    "# print('train data of user_id shape',len(set(train['user_id'])))\n",
    "# print('train data of current_service shape',(set(train['current_service'])))\n",
    "\n",
    "# print('train data shape',test.shape)\n",
    "# print('train data of user_id shape',len(set(test['user_id'])))\n",
    "\n",
    "# 对标签编码 映射关系\n",
    "label2current_service = dict(zip(range(0,len(set(train['current_service']))),sorted(list(set(train['current_service'])))))\n",
    "current_service2label = dict(zip(sorted(list(set(train['current_service']))),range(0,len(set(train['current_service'])))))\n",
    "\n",
    "# 原始数据的标签映射\n",
    "train['current_service'] = train['current_service'].map(current_service2label)\n",
    "    \n",
    "# 构造原始数据\n",
    "y = train.pop('current_service')\n",
    "train_id = train.pop('user_id')\n",
    "# 这个字段有点问题\n",
    "X = train\n",
    "train_col = train.columns\n",
    "\n",
    "X_test = test[train_col]\n",
    "test_id = test['user_id']\n",
    "\n",
    "# 数据有问题数据\n",
    "for i in train_col:\n",
    "    X[i] = X[i].replace(\"\\\\N\",-1)\n",
    "    X_test[i] = X_test[i].replace(\"\\\\N\",-1)\n",
    "\n",
    "X,y,X_test = X.values,y,X_test.values\n",
    "\n",
    "#数据拆分\n",
    "print('数据拆分')\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=21)\n",
    "\n",
    "### 数据转换\n",
    "print('数据转换')\n",
    "lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train,free_raw_data=False)\n",
    "\n",
    "### 设置初始参数--不含交叉验证参数\n",
    "print('设置参数')\n",
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective' : 'multiclass',\n",
    "            'num_class' : 15,\n",
    "            'num_boost_round' : 3000,\n",
    "            'metric': 'binary_logloss',\n",
    "          }\n",
    "\n",
    "\n",
    "### 交叉验证(调参)\n",
    "print('交叉验证')\n",
    "min_merror = float('Inf')\n",
    "best_params = {}\n",
    "\n",
    "# 准确率\n",
    "print(\"调参1：提高准确率\")\n",
    "for num_leaves in range(20,40):\n",
    "    for max_depth in range(3,6):\n",
    "        params['num_leaves'] = num_leaves\n",
    "        params['max_depth'] = max_depth\n",
    "\n",
    "        cv_results = lgb.cv(\n",
    "                            params,\n",
    "                            lgb_train,\n",
    "                            seed=201,\n",
    "                            nfold=3,\n",
    "                            metrics=['binary_logloss'],\n",
    "                            early_stopping_rounds=20,\n",
    "                            verbose_eval=150,\n",
    "                            show_stdv = True\n",
    "                            )\n",
    "            \n",
    "        mean_merror = pd.Series(cv_results['binary_logloss-mean']).min()\n",
    "        boost_rounds = pd.Series(cv_results['binary_logloss-mean']).argmin()\n",
    "            \n",
    "        if mean_merror < min_merror:\n",
    "            min_merror = mean_merror\n",
    "            best_params['num_leaves'] = num_leaves\n",
    "            best_params['max_depth'] = max_depth\n",
    "            \n",
    "params['num_leaves'] = best_params['num_leaves']\n",
    "params['max_depth'] = best_params['max_depth']\n",
    "\n",
    "# 过拟合\n",
    "print(\"调参2：降低过拟合\")\n",
    "for max_bin in range(5,255):\n",
    "    for min_data_in_leaf in range(10,200):\n",
    "            params['max_bin'] = max_bin\n",
    "            params['min_data_in_leaf'] = min_data_in_leaf\n",
    "            \n",
    "            cv_results = lgb.cv(\n",
    "                                params,\n",
    "                                lgb_train,\n",
    "                                seed=42,\n",
    "                                nfold=3,\n",
    "                                metrics=['binary_logloss'],\n",
    "                                early_stopping_rounds=10,\n",
    "                                verbose_eval=True\n",
    "                                )\n",
    "                    \n",
    "            mean_merror = pd.Series(cv_results['binary_logloss-mean']).min()\n",
    "            boost_rounds = pd.Series(cv_results['binary_logloss-mean']).argmin()\n",
    "\n",
    "            if mean_merror < min_merror:\n",
    "                min_merror = mean_merror\n",
    "                best_params['max_bin']= max_bin\n",
    "                best_params['min_data_in_leaf'] = min_data_in_leaf\n",
    "\n",
    "params['min_data_in_leaf'] = best_params['min_data_in_leaf']\n",
    "params['max_bin'] = best_params['max_bin']\n",
    "\n",
    "print(\"调参3：降低过拟合\")\n",
    "for feature_fraction in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    for bagging_fraction in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "        for bagging_freq in range(0,50,5):\n",
    "            params['feature_fraction'] = feature_fraction\n",
    "            params['bagging_fraction'] = bagging_fraction\n",
    "            params['bagging_freq'] = bagging_freq\n",
    "            \n",
    "            cv_results = lgb.cv(\n",
    "                                params,\n",
    "                                lgb_train,\n",
    "                                seed=42,\n",
    "                                nfold=3,\n",
    "                                metrics=['binary_logloss'],\n",
    "                                early_stopping_rounds=3,\n",
    "                                verbose_eval=True\n",
    "                                )\n",
    "                    \n",
    "            mean_merror = pd.Series(cv_results['binary_logloss-mean']).min()\n",
    "            boost_rounds = pd.Series(cv_results['binary_logloss-mean']).argmin()\n",
    "\n",
    "            if mean_merror < min_merror:\n",
    "                min_merror = mean_merror\n",
    "                best_params['feature_fraction'] = feature_fraction\n",
    "                best_params['bagging_fraction'] = bagging_fraction\n",
    "                best_params['bagging_freq'] = bagging_freq\n",
    "\n",
    "params['feature_fraction'] = best_params['feature_fraction']\n",
    "params['bagging_fraction'] = best_params['bagging_fraction']\n",
    "params['bagging_freq'] = best_params['bagging_freq']\n",
    "\n",
    "print(\"调参4：降低过拟合\")\n",
    "for lambda_l1 in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    for lambda_l2 in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "        for min_split_gain in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "            params['lambda_l1'] = lambda_l1\n",
    "            params['lambda_l2'] = lambda_l2\n",
    "            params['min_split_gain'] = min_split_gain\n",
    "            \n",
    "            cv_results = lgb.cv(\n",
    "                                params,\n",
    "                                lgb_train,\n",
    "                                seed=42,\n",
    "                                nfold=3,\n",
    "                                metrics=['binary_logloss'],\n",
    "                                early_stopping_rounds=3,\n",
    "                                verbose_eval=True\n",
    "                                )\n",
    "                    \n",
    "            mean_merror = pd.Series(cv_results['binary_logloss-mean']).min()\n",
    "            boost_rounds = pd.Series(cv_results['binary_logloss-mean']).argmin()\n",
    "\n",
    "            if mean_merror < min_merror:\n",
    "                min_merror = mean_merror\n",
    "                best_params['lambda_l1'] = lambda_l1\n",
    "                best_params['lambda_l2'] = lambda_l2\n",
    "                best_params['min_split_gain'] = min_split_gain\n",
    "\n",
    "params['lambda_l1'] = best_params['lambda_l1']\n",
    "params['lambda_l2'] = best_params['lambda_l2']\n",
    "params['min_split_gain'] = best_params['min_split_gain']\n",
    "\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "### 训练\n",
    "params['learning_rate']=0.01\n",
    "lgb.train(\n",
    "          params,                     # 参数字典\n",
    "          lgb_train,                  # 训练集\n",
    "          valid_sets=lgb_eval,        # 验证集\n",
    "          num_boost_round=2000,       # 迭代次数\n",
    "          early_stopping_rounds=50    # 早停次数\n",
    "          )\n",
    "\n",
    "### 保存模型\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lgb,path +'lgb.pkl')\n",
    "\n",
    "### 特征选择\n",
    "df = pd.DataFrame(X_train.columns.tolist(), columns=['feature'])\n",
    "df['importance']=list(lgb.feature_importance())                           # 特征分数\n",
    "df = df.sort_values(by='importance',ascending=False)                      # 特征排序\n",
    "df.to_csv(path +\"feature_score_20180919.csv\",index=None,encoding='gbk')  # 保存分数\n",
    "\n",
    "\n",
    "# 采取k折模型方案\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "xx_score = []\n",
    "cv_pred = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits,random_state=seed,shuffle=True)\n",
    "for index,(train_index,test_index) in enumerate(skf.split(X,y)):\n",
    "    print(index)\n",
    "\n",
    "    X_train,X_valid,y_train,y_valid = X[train_index],X[test_index],y[train_index],y[test_index]\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    validation_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "    clf=lgb.train(params,train_data,num_boost_round=100000,valid_sets=[validation_data],early_stopping_rounds=50,feval=f1_score_vali,verbose_eval=1)\n",
    "\n",
    "    xx_pred = clf.predict(X_valid,num_iteration=clf.best_iteration)\n",
    "\n",
    "    xx_pred = [np.argmax(x) for x in xx_pred]\n",
    "\n",
    "    xx_score.append(f1_score(y_valid,xx_pred,average='weighted'))\n",
    "\n",
    "    y_test = clf.predict(X_test,num_iteration=clf.best_iteration)\n",
    "\n",
    "    y_test = [np.argmax(x) for x in y_test]\n",
    "\n",
    "    if index == 0:\n",
    "        cv_pred = np.array(y_test).reshape(-1, 1)\n",
    "    else:\n",
    "        cv_pred = np.hstack((cv_pred, np.array(y_test).reshape(-1, 1)))\n",
    "\n",
    "# 投票\n",
    "submit = []\n",
    "for line in cv_pred:\n",
    "    submit.append(np.argmax(np.bincount(line)))\n",
    "\n",
    "# 保存结果\n",
    "df_test = pd.DataFrame()\n",
    "df_test['id'] = list(test_id.unique())\n",
    "df_test['predict'] = submit\n",
    "df_test['predict'] = df_test['predict'].map(label2current_service)\n",
    "\n",
    "df_test.to_csv(path +'result/baseline2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
